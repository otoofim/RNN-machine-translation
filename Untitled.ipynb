{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size, en=3713, fr=3949\n",
      "--------------------------------------------------\n",
      "Training progress will be logged in:\n",
      "\tmodel/train_10000sen_1-1layers_100units_ja_en_exp1_NO_ATTN.log\n",
      "--------------------------------------------------\n",
      "Trained model will be saved as:\n",
      "\tmodel/seq2seq_10000sen_1-1layers_100units_ja_en_exp1_NO_ATTN.model\n",
      "--------------------------------------------------\n",
      "Existing model found\n",
      "--------------------------------------------------\n",
      "loading model ...\n",
      "finished loading: model/seq2seq_10000sen_1-1layers_100units_ja_en_exp1_NO_ATTN.model\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "#---------------------------------------------------------------------\n",
    "'''\n",
    "Neural Machine Translation - Translation experiments\n",
    "    Training, evaluation and prediction functions\n",
    "'''\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import os\n",
    "from collections import Counter\n",
    "import math\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "import importlib\n",
    "# %matplotlib inline\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Load configuration\n",
    "#---------------------------------------------------------------------\n",
    "from nmt_config import *\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Load encoder decoder model definition\n",
    "#---------------------------------------------------------------------\n",
    "from enc_dec import *\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "xp = cuda.cupy if gpuid >= 0 else np\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Load dataset\n",
    "#---------------------------------------------------------------------\n",
    "w2i = pickle.load(open(w2i_path, \"rb\"))\n",
    "i2w = pickle.load(open(i2w_path, \"rb\"))\n",
    "vocab = pickle.load(open(vocab_path, \"rb\"))\n",
    "vocab_size_en = min(len(i2w[\"en\"]), max_vocab_size[\"en\"])\n",
    "vocab_size_fr = min(len(i2w[\"fr\"]), max_vocab_size[\"fr\"])\n",
    "print(\"vocab size, en={0:d}, fr={1:d}\".format(vocab_size_en, vocab_size_fr))\n",
    "print(\"{0:s}\".format(\"-\"*50))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Set up model\n",
    "#---------------------------------------------------------------------\n",
    "model = EncoderDecoder(vocab_size_fr, vocab_size_en,\n",
    "                       num_layers_enc, num_layers_dec,\n",
    "                       hidden_units, gpuid, attn=use_attn)\n",
    "if gpuid >= 0:\n",
    "    cuda.get_device(gpuid).use()\n",
    "    model.to_gpu()\n",
    "\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "'''\n",
    "___QUESTION-1-DESCRIBE-F-START___\n",
    "\n",
    "- Describe what the following line of code does\n",
    "The following code add gradient descent optimizer with regularization term (L2 norm) to our model to update it in each itration.\n",
    "'''\n",
    "optimizer.add_hook(chainer.optimizer.GradientClipping(threshold=5))\n",
    "'''___QUESTION-1-DESCRIBE-F-END___'''\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "print(\"Training progress will be logged in:\\n\\t{0:s}\".format(log_train_fil_name))\n",
    "print(\"{0:s}\".format(\"-\"*50))\n",
    "print(\"Trained model will be saved as:\\n\\t{0:s}\".format(model_fil))\n",
    "print(\"{0:s}\".format(\"-\"*50))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Evaluation utilities\n",
    "#---------------------------------------------------------------------\n",
    "# Compute perplexity over validation set\n",
    "def compute_dev_pplx():\n",
    "    loss = 0\n",
    "    num_words = 0\n",
    "    num_sents = 0\n",
    "    with open(text_fname[\"fr\"], \"rb\") as fr_file, open(text_fname[\"en\"], \"rb\") as en_file:\n",
    "        with tqdm(total=NUM_DEV_SENTENCES) as pbar:\n",
    "            sys.stderr.flush()\n",
    "            out_str = \"loss={0:.6f}\".format(0)\n",
    "            pbar.set_description(out_str)\n",
    "            for i, (line_fr, line_en) in enumerate(zip(fr_file, en_file), start=1):\n",
    "                if i > NUM_TRAINING_SENTENCES and i <= (NUM_TRAINING_SENTENCES + NUM_DEV_SENTENCES):\n",
    "                    fr_sent = line_fr.strip().split()\n",
    "                    en_sent = line_en.strip().split()\n",
    "\n",
    "                    fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "                    en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "                    # compute loss\n",
    "                    curr_loss = float(model.encode_decode_train(fr_ids, en_ids, train=False).data)  / len(en_ids)\n",
    "                    loss += curr_loss\n",
    "                    num_words += len(en_ids)\n",
    "\n",
    "                    out_str = \"loss={0:.6f}\".format(curr_loss)\n",
    "                    pbar.set_description(out_str)\n",
    "                    pbar.update(1)\n",
    "                    num_sents += 1\n",
    "                \n",
    "            # end for\n",
    "        # end pbar\n",
    "    # end with open file\n",
    "    #loss_per_word = loss / num_words\n",
    "    loss_per_sentence = loss / num_sents\n",
    "    pplx = 2 ** loss_per_sentence\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:14s} | {1:.4f}\".format(\"dev perplexity\", pplx))\n",
    "    print(\"{0:14s} | {1:d}\".format(\"# words in dev\", num_words))\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    return pplx\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Compute Bleu score\n",
    "#---------------------------------------------------------------------\n",
    "def bleu_stats(hypothesis, reference):\n",
    "    yield len(hypothesis)\n",
    "    yield len(reference)\n",
    "    for n in range(1,5):\n",
    "        s_ngrams = Counter([tuple(hypothesis[i:i+n]) for i in range(len(hypothesis)+1-n)])\n",
    "        r_ngrams = Counter([tuple(reference[i:i+n]) for i in range(len(reference)+1-n)])\n",
    "        yield max([sum((s_ngrams & r_ngrams).values()), 0])\n",
    "        yield max([len(hypothesis)+1-n, 0])\n",
    "\n",
    "# Compute BLEU from collected statistics obtained by call(s) to bleu_stats\n",
    "def bleu(stats):\n",
    "    if len(list(filter(lambda x: x==0, stats))) > 0:\n",
    "        return 0\n",
    "    (c, r) = stats[:2]\n",
    "    log_bleu_prec = sum([math.log(float(x)/y) for x,y in zip(stats[2::2],stats[3::2])]) / 4.\n",
    "    return math.exp(min([0, 1-float(r)/c]) + log_bleu_prec)\n",
    "\n",
    "def compute_dev_bleu():\n",
    "    list_of_references = []\n",
    "    list_of_hypotheses = []\n",
    "    with open(text_fname[\"fr\"], \"rb\") as fr_file, open(text_fname[\"en\"], \"rb\") as en_file:\n",
    "        for i, (line_fr, line_en) in enumerate(zip(fr_file, en_file), start=1):\n",
    "            if i > NUM_TRAINING_SENTENCES and i <= (NUM_TRAINING_SENTENCES + NUM_DEV_SENTENCES):\n",
    "                fr_sent = line_fr.strip().split()\n",
    "                en_sent = line_en.strip().split()\n",
    "\n",
    "                fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "                en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "                list_of_references.append(line_en.strip().decode())\n",
    "                pred_sent, alpha_arr = model.encode_decode_predict(fr_ids, max_predict_len=MAX_PREDICT_LEN)\n",
    "                pred_words = [i2w[\"en\"][w].decode() for w in pred_sent if w != EOS_ID]\n",
    "                pred_sent_line = \" \".join(pred_words)\n",
    "                list_of_hypotheses.append(pred_sent_line)\n",
    "            if i > (NUM_TRAINING_SENTENCES + NUM_DEV_SENTENCES):\n",
    "                break\n",
    "\n",
    "    stats = [0 for i in range(10)]\n",
    "    for (r,h) in zip(list_of_references, list_of_hypotheses):\n",
    "        stats = [sum(scores) for scores in zip(stats, bleu_stats(h,r))]\n",
    "\n",
    "    bleu_score = (100 * bleu(stats))\n",
    "    print(\"BLEU: {0:0.3f}\".format(bleu_score))\n",
    "    return bleu_score\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Main training loop\n",
    "#---------------------------------------------------------------------\n",
    "def train_loop(text_fname, num_training, num_epochs, log_mode=\"a\"):\n",
    "    # Set up log file for loss\n",
    "    log_train_fil = open(log_train_fil_name, mode=log_mode)\n",
    "    log_train_csv = csv.writer(log_train_fil, lineterminator=\"\\n\")\n",
    "\n",
    "    sys.stderr.flush()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with open(text_fname[\"fr\"], \"rb\") as fr_file, open(text_fname[\"en\"], \"rb\") as en_file:\n",
    "            with tqdm(total=num_training) as pbar:\n",
    "                sys.stderr.flush()\n",
    "                loss_per_epoch = 0\n",
    "                out_str = \"epoch={0:d}, iter={1:d}, loss={2:.6f}, mean loss={3:.6f}\".format(\n",
    "                                epoch+1, 0, 0, 0)\n",
    "                pbar.set_description(out_str)\n",
    "\n",
    "                for i, (line_fr, line_en) in enumerate(zip(fr_file, en_file), start=1):\n",
    "                    fr_sent = line_fr.strip().split()\n",
    "                    en_sent = line_en.strip().split()\n",
    "\n",
    "                    fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "                    en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "                    it = (epoch * NUM_TRAINING_SENTENCES) + i\n",
    "\n",
    "                    if i > num_training:\n",
    "                        break\n",
    "\n",
    "                    # compute loss\n",
    "                    loss = model.encode_decode_train(fr_ids, en_ids)\n",
    "\n",
    "                    # clear gradient\n",
    "                    model.cleargrads()\n",
    "                    # backprop\n",
    "                    loss.backward()\n",
    "                    # update parameters\n",
    "                    optimizer.update()\n",
    "                    # store loss value for display\n",
    "                    loss_val = float(loss.data) / len(en_ids)\n",
    "                    loss_per_epoch += loss_val\n",
    "\n",
    "                    out_str = \"epoch={0:d}, iter={1:d}, loss={2:.6f}, mean loss={3:.6f}\".format(\n",
    "                               epoch+1, it, loss_val, (loss_per_epoch / i))\n",
    "                    pbar.set_description(out_str)\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # log every 100 sentences\n",
    "                    if i % 100 == 0:\n",
    "                        log_train_csv.writerow([it, loss_val])\n",
    "\n",
    "\n",
    "        # compute precision, recall and F-score\n",
    "        metrics = predict(s=NUM_TRAINING_SENTENCES, \n",
    "                         num=NUM_DEV_SENTENCES, display=False, plot=False)\n",
    "        prec = np.sum(metrics[\"cp\"]) / np.sum(metrics[\"tp\"])\n",
    "        rec = np.sum(metrics[\"cp\"]) / np.sum(metrics[\"t\"])\n",
    "        f_score = 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"{0:10s} | {1:0.4f}\".format(\"precision\", prec))\n",
    "        print(\"{0:10s} | {1:0.4f}\".format(\"recall\", rec))\n",
    "        print(\"{0:10s} | {1:0.4f}\".format(\"f1\", f_score))\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"computing perplexity\")\n",
    "        pplx = compute_dev_pplx()\n",
    "\n",
    "        # Backup model every epoch\n",
    "        print(\"Saving model\")\n",
    "        serializers.save_npz(model_fil, model)\n",
    "        print(\"Finished saving model\")\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "        # Compute Bleu every 2 epochs\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"computing bleu\")\n",
    "            bleu_score = compute_dev_bleu()\n",
    "            print(\"finished computing bleu ... \")\n",
    "            print(\"{0:s}\".format(\"-\"*50))\n",
    "        \n",
    "    # At the end of training, make some predictions\n",
    "    # make predictions over both training and dev sets\n",
    "    print(\"Training set predictions\")\n",
    "    _ = predict(s=0, num=3, plot=False)\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"dev set predictions\")\n",
    "    _ = predict(s=NUM_TRAINING_SENTENCES, num=3, plot=False)\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    # Check if Bleu needs to be recomputed\n",
    "    if num_epochs % 2 != 0:\n",
    "        bleu_score = compute_dev_bleu()\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "    # close log file\n",
    "    log_train_fil.close()\n",
    "    print(\"Finished training. Filenames:\")\n",
    "    print(log_train_fil_name)\n",
    "    print(model_fil)\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# __QUESTION -- Following code is to assist with ATTENTION\n",
    "#---------------------------------------------------------------------\n",
    "from matplotlib.font_manager import FontProperties\n",
    "'''\n",
    "Support function to plot attention vectors\n",
    "'''\n",
    "def plot_attention(alpha_arr, fr, en, plot_name=None):\n",
    "    if gpuid >= 0:\n",
    "        alpha_arr = cuda.to_cpu(alpha_arr).astype(np.float32)\n",
    "\n",
    "    #alpha_arr /= np.max(np.abs(alpha_arr),axis=0)\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(8, 8)\n",
    "\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[12,1],height_ratios=[12,1])\n",
    "\n",
    "    ax = plt.subplot(gs[0])\n",
    "    ax_c = plt.subplot(gs[1])\n",
    "\n",
    "    cmap = sns.light_palette((200, 75, 60), input=\"husl\", as_cmap=True)\n",
    "    prop = FontProperties(fname='fonts/IPAfont00303/ipam.ttf', size=12)\n",
    "    ax = sns.heatmap(alpha_arr, xticklabels=fr, yticklabels=en, ax=ax, cmap=cmap, cbar_ax=ax_c)\n",
    "\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.yaxis.tick_right()\n",
    "\n",
    "    ax.set_xticklabels(en, minor=True, rotation=60, size=12)\n",
    "    for label in ax.get_xticklabels(minor=False):\n",
    "        label.set_fontsize(12)\n",
    "        label.set_font_properties(prop)\n",
    "\n",
    "    for label in ax.get_yticklabels(minor=False):\n",
    "        label.set_fontsize(12)\n",
    "        label.set_rotation(-90)\n",
    "        label.set_horizontalalignment('left')\n",
    "\n",
    "    ax.set_xlabel(\"Source\", size=20)\n",
    "    ax.set_ylabel(\"Hypothesis\", size=20)\n",
    "\n",
    "    if plot_name:\n",
    "        fig.savefig(plot_name, format=\"png\")\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Helper function for prediction\n",
    "#---------------------------------------------------------------------\n",
    "def predict_sentence(line_fr, line_en=None, display=True, \n",
    "                     plot_name=None, p_filt=0, r_filt=0, sample=False):\n",
    "    fr_sent = line_fr.strip().split()\n",
    "    fr_ids = [w2i[\"fr\"].get(w, UNK_ID) for w in fr_sent]\n",
    "    # english reference is optional. If provided, compute precision/recall\n",
    "    if line_en:\n",
    "        en_sent = line_en.strip().split()\n",
    "        en_ids = [w2i[\"en\"].get(w, UNK_ID) for w in en_sent]\n",
    "\n",
    "    pred_ids, alpha_arr = model.encode_decode_predict(fr_ids, \n",
    "                                                      max_predict_len=MAX_PREDICT_LEN,\n",
    "                                                      sample=sample)\n",
    "    pred_words = [i2w[\"en\"][w].decode() for w in pred_ids]\n",
    "\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    filter_match = False\n",
    "\n",
    "    matches = count_match(en_ids, pred_ids)\n",
    "    prec = matches/len(pred_ids)\n",
    "    rec = matches/len(en_ids)\n",
    "\n",
    "    if display and (prec >= p_filt and rec >= r_filt):\n",
    "        filter_match = True\n",
    "        # convert raw binary into string\n",
    "        fr_words = [w.decode() for w in fr_sent]\n",
    "\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Src\", line_fr.strip().decode()))\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Ref\", line_en.strip().decode()))\n",
    "        print(\"{0:s} | {1:80s}\".format(\"Hyp\", \" \".join(pred_words)))\n",
    "\n",
    "        print(\"{0:s}\".format(\"-\"*50))\n",
    "\n",
    "        print(\"{0:s} | {1:0.4f}\".format(\"precision\", prec))\n",
    "        print(\"{0:s} | {1:0.4f}\".format(\"recall\", rec))\n",
    "\n",
    "        if plot_name and use_attn:\n",
    "            plot_attention(alpha_arr, fr_words, pred_words, plot_name)\n",
    "\n",
    "    return matches, len(pred_ids), len(en_ids), filter_match\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "'''\n",
    "Function to make predictions.\n",
    "s       : starting index of the line in the parallel data from which to make predictions\n",
    "num     : number of lines starting from \"s\" to make predictions for\n",
    "plot    : plot attention if True\n",
    "display : whether to display additional info\n",
    "p_filt  : precision filter. Only displays predicted sentences with precision above p_filt\n",
    "r_filt  : recall filter. Only displays predicted sentences with recall above p_filt\n",
    "sample  : if False, predict word with maximum probability in the model, else sample\n",
    "'''\n",
    "#---------------------------------------------------------------------\n",
    "def predict(s=NUM_TRAINING_SENTENCES, num=NUM_DEV_SENTENCES, \n",
    "            display=True, plot=False, p_filt=0, r_filt=0, sample=False):\n",
    "    \n",
    "    if display:\n",
    "        print(\"English predictions, s={0:d}, num={1:d}:\".format(s, num))\n",
    "\n",
    "    metrics = {\"cp\":[], \"tp\":[], \"t\":[]}\n",
    "\n",
    "    filter_count = 0\n",
    "\n",
    "    with open(text_fname[\"fr\"], \"rb\") as fr_file, open(text_fname[\"en\"], \"rb\") as en_file:\n",
    "        for i, (line_fr, line_en) in enumerate(zip(fr_file, en_file), start=0):\n",
    "            if i >= s and i < (s+num):\n",
    "                if plot:\n",
    "                    plot_name = os.path.join(model_dir, \"sample_{0:d}_plot.png\".format(i+1))\n",
    "                else:\n",
    "                    plot_name=None\n",
    "\n",
    "                # make prediction\n",
    "                cp, tp, t, f = predict_sentence(line_fr,\n",
    "                                             line_en,\n",
    "                                             display=display,\n",
    "                                             plot_name=plot_name, \n",
    "                                             p_filt=p_filt, r_filt=r_filt,\n",
    "                                             sample=sample)\n",
    "                metrics[\"cp\"].append(cp)\n",
    "                metrics[\"tp\"].append(tp)\n",
    "                metrics[\"t\"].append(t)\n",
    "                filter_count += (1 if f else 0)\n",
    "\n",
    "    if display:\n",
    "        print(\"sentences matching filter = {0:d}\".format(filter_count))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "#---------------------------------------------------------------------\n",
    "# Helper function to compute precision recall\n",
    "#---------------------------------------------------------------------\n",
    "def count_match(list1, list2):\n",
    "    # each list can have repeated elements. The count should account for this.\n",
    "    count1 = Counter(list1)\n",
    "    count2 = Counter(list2)\n",
    "    count2_keys = count2.keys()-set([UNK_ID])\n",
    "    common_w = set(count1.keys()) & set(count2_keys)\n",
    "    matches = sum([min(count1[w], count2[w]) for w in common_w])\n",
    "    return matches\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# Main -- program execution starts here\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"Existing model {0:s}\".format(\"found\" if os.path.exists(model_fil) else \"not found!\"))\n",
    "    print(\"{0:s}\".format(\"-\"*50))\n",
    "    if os.path.exists(model_fil):\n",
    "        if load_existing_model:\n",
    "            print(\"loading model ...\")\n",
    "            serializers.load_npz(model_fil, model)\n",
    "            print(\"finished loading: {0:s}\".format(model_fil))\n",
    "            print(\"{0:s}\".format(\"-\"*50))\n",
    "        else:\n",
    "            print(\"\"\"model file already exists!!\n",
    "                Delete before continuing, or enable load_existing flag\"\"\".format(model_fil))\n",
    "            return\n",
    "\n",
    "    if NUM_EPOCHS > 0:\n",
    "        train_loop(text_fname, NUM_TRAINING_SENTENCES, NUM_EPOCHS)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "def test_lam_tran():\n",
    "    lines_fr = [\"君 は １ 日 で それ が でき ま す か 。\", \"皮肉 な 笑い を 浮かべ て 彼 は 私 を 見つめ た 。\", \"あなた は 午後 何 を し た い で す か 。\"]\n",
    "    lines_en = [\"can you do it in one day ?\", \"he stared at me with a satirical smile .\", \"what do you want to do in the afternoon ?\" ]\n",
    "\n",
    "    for line_fr, line_en in zip(lines_fr, lines_en):\n",
    "        line_fr = line_fr.encode()\n",
    "        line_en = line_en.encode()\n",
    "\n",
    "        predict_sentence(line_fr=line_fr, line_en=line_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English predictions, s=10000, num=10:\n",
      "sentences matching filter = 0\n"
     ]
    }
   ],
   "source": [
    "_ = predict(s=10000, num=10, p_filt=.3, sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English predictions, s=10000, num=10:\n",
      "--------------------------------------------------\n",
      "Src | この 路地 は 通り抜け でき ま せ ん 。                                                         \n",
      "Ref | this is a dead @-@ end alley .                                                  \n",
      "Hyp | this can &apos;t this this . . _EOS                                             \n",
      "--------------------------------------------------\n",
      "precision | 0.2500\n",
      "recall | 0.2500\n",
      "--------------------------------------------------\n",
      "Src | ええ 、 届 い た の を お 知 らせ する の を 忘れ て しま っ て す み ま せ ん 。                            \n",
      "Ref | yes , sorry , i forgot to acknowledge it .                                      \n",
      "Hyp | you don &apos;t forget to when you , parents advice _UNK you to . . . _EOS      \n",
      "--------------------------------------------------\n",
      "precision | 0.1765\n",
      "recall | 0.3000\n",
      "--------------------------------------------------\n",
      "Src | 彼 は ドイツ 生まれ の 人 だ 。                                                             \n",
      "Ref | he is a german by origin .                                                      \n",
      "Hyp | he is a man person . . _EOS                                                     \n",
      "--------------------------------------------------\n",
      "precision | 0.5000\n",
      "recall | 0.5714\n",
      "--------------------------------------------------\n",
      "Src | 我々 は ロビン フッド の 伝説 を 良く 知 っ て い る 。                                              \n",
      "Ref | we are familiar with the legend of robin hood .                                 \n",
      "Hyp | we are looking all - peace the . . _EOS                                         \n",
      "--------------------------------------------------\n",
      "precision | 0.4000\n",
      "recall | 0.4000\n",
      "--------------------------------------------------\n",
      "Src | トランク に は 鍵 が かけ られ て い ま す か 。                                                  \n",
      "Ref | is your trunk locked ?                                                          \n",
      "Hyp | is there _UNK _UNK _UNK _UNK _UNK ? ? _EOS                                      \n",
      "--------------------------------------------------\n",
      "precision | 0.2000\n",
      "recall | 0.4000\n",
      "--------------------------------------------------\n",
      "Src | それ は 冗談 の つもり で い っ た だけ だ よ 。                                                  \n",
      "Ref | i just meant it as a joke .                                                     \n",
      "Hyp | it will go my to as a . . _EOS                                                  \n",
      "--------------------------------------------------\n",
      "precision | 0.4000\n",
      "recall | 0.5000\n",
      "--------------------------------------------------\n",
      "Src | その 部屋 は 居心地 の 良 い 感じ が し た 。                                                    \n",
      "Ref | the room had a nice cozy feel .                                                 \n",
      "Hyp | the town was was the . . _EOS                                                   \n",
      "--------------------------------------------------\n",
      "precision | 0.2500\n",
      "recall | 0.2500\n",
      "--------------------------------------------------\n",
      "Src | パソコン が 使え る の は 彼女 だけ で す 。                                                     \n",
      "Ref | she alone is able to use the personal computer .                                \n",
      "Hyp | the moon is to is . . . _EOS                                                    \n",
      "--------------------------------------------------\n",
      "precision | 0.4444\n",
      "recall | 0.4000\n",
      "--------------------------------------------------\n",
      "Src | 彼 ら は 共同 声明 に 同意 し た 。                                                          \n",
      "Ref | they agreed on a joint statement .                                              \n",
      "Hyp | they forced on the the . . _EOS                                                 \n",
      "--------------------------------------------------\n",
      "precision | 0.3750\n",
      "recall | 0.4286\n",
      "--------------------------------------------------\n",
      "Src | 彼 は 彼女 を バス に 助け あげ た 。                                                         \n",
      "Ref | he handed her up into the bus .                                                 \n",
      "Hyp | he _UNK her her her her her . _EOS                                              \n",
      "--------------------------------------------------\n",
      "precision | 0.3333\n",
      "recall | 0.3750\n",
      "sentences matching filter = 10\n"
     ]
    }
   ],
   "source": [
    "_ = predict(s=10000, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False  True]\n",
      "[0 3 0 0 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[2,3,4,5,6]\n",
    "b=np.random.uniform(size=[5])<0.5\n",
    "print(b)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mtenv]",
   "language": "python",
   "name": "conda-env-mtenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
